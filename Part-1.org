* Linear Regression
**  Siraj live - linear regression ([[https://github.com/llSourcell/linear_regression_live/blob/master/demo.py][Code]])

* Neural Nets
 + [[http://karpathy.github.io/neuralnets/][Best Explaination on NN and Backprop]]
** Implementing Simplest Neural Net
   + This network is single layered so it doesn't even need backprop for training
   #+BEGIN_SRC ipython :session mysession :exports both
import numpy as np

def sigmoid(x):
    # TODO: Implement sigmoid function
    return 1/(1+ np.exp(-x))

inputs = np.array([0.7, -0.3])
weights = np.array([0.1, 0.8])
bias = -0.1
# TODO: Calculate the output
output = sigmoid(np.dot(weights,inputs) + bias)

(output)

   #+END_SRC

** Gradient descent
   + Momentum prevents GD from converging at local optima by taking advantage from knowledge of previous steps about where we should be headed
   + An advantage of using sigmoid as activation function is that gradient (derivative) can be written in terms of the sigmoid function itself
   + This network is single layered so it doesn't even need backprop for training
   #+BEGIN_SRC ipython :session mysession :exports both
import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

learnrate = 0.5
x = np.array([1, 2])
y = np.array(0.5)

# Initial weights
w = np.array([0.5, -0.5])

# Calculate one gradient descent step for each weight
# TODO: Calculate output of neural network
nn_output = sigmoid(np.dot(x,w))

# TODO: Calculate error of neural network
error = y - nn_output

# TODO: Calculate change in weights
del_w = learnrate * error*sigmoid_prime(np.dot(x,w)) * x

print('Neural Network output:')
print(nn_output)
print('Amount of Error:')
print(error)
print('Change in Weights:')
print(del_w)
   #+END_SRC

** Implement Gradient descent
    #+BEGIN_SRC ipython :session mysession :exports both
import numpy as np
from data_prep import features, targets, features_test, targets_test


def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Use to same seed to make debugging easier
np.random.seed(42)

n_records, n_features = features.shape
last_loss = None

# Initialize weights
weights = np.random.normal(scale=1 / n_features**.5, size=n_features)

# Neural Network hyperparameters
epochs = 1000
learnrate = 0.5

for e in range(epochs):
    del_w = np.zeros(weights.shape)
    for x, y in zip(features.values, targets):
        # Loop through all records, x is the input, y is the target

        # TODO: Calculate the output
        output = sigmoid(np.dot(weights, x))

        # TODO: Calculate the error
        error = y-output

        # TODO: Calculate change in weights
        del_w += error*sigmoid_prime(np.dot(x,weights)) * x

        # TODO: Update weights
    weights += learnrate * del_w / n_records

    # Printing out the mean square error on the training set
    if e % (epochs / 10) == 0:
        out = sigmoid(np.dot(features, weights))
        loss = np.mean((out - targets) ** 2)
        if last_loss and last_loss < loss:
            print("Train loss: ", loss, "  WARNING - Loss Increasing")
        else:
            print("Train loss: ", loss)
        last_loss = loss


# Calculate accuracy on test data
tes_out = sigmoid(np.dot(features_test, weights))
predictions = tes_out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))
    #+END_SRC

** Implementing Multi-layer perceptrons
   #+BEGIN_SRC ipython :session mysession :exports both
import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

# Network size
N_input = 4
N_hidden = 3
N_output = 2

np.random.seed(42)
# Make some fake data
X = np.random.randn(4)

weights_in_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_out = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))


# TODO: Make a forward pass through the network

hidden_layer_in = np.dot(X,weights_in_hidden)
hidden_layer_out = sigmoid(hidden_layer_in)

print('Hidden-layer Output:')
print(hidden_layer_out)

output_layer_in = np.dot(hidden_layer_out,weights_hidden_out)
output_layer_out = sigmoid(output_layer_in)

print('Output-layer Output:')
print(output_layer_out)
   #+END_SRC

   #+RESULTS:


** Backprop
   + Resource:
     1. From Andrej Kaparthy: [[https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9][Yes, you should understand backprop]]
     2. Also from Andrej Kaparthy, [[https://www.youtube.com/watch?v%3D59Hbtz7XgjM][a lecture from Stanford's CS231n course]]
   + Code:
#+BEGIN_SRC ipython :session mysession :exports both
import numpy as np


def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))


x = np.array([0.5, 0.1, -0.2])
target = 0.6
learnrate = 0.5

weights_input_hidden = np.array([[0.5, -0.6],
                                 [0.1, -0.2],
                                 [0.1, 0.7]])

weights_hidden_output = np.array([0.1, -0.3])

## Forward pass
hidden_layer_input = np.dot(x, weights_input_hidden)
hidden_layer_output = sigmoid(hidden_layer_input)

output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)
output = sigmoid(output_layer_in)

## Backwards pass
## TODO: Calculate error
error = output-target

# TODO: Calculate error gradient for output layer
del_err_output = sigmoid(output_layer_in)*(1-sigmoid(output_layer_in)) * error

# TODO: Calculate error gradient for hidden layer
del_err_hidden = sigmoid(hidden_layer_input)*(1-sigmoid(hidden_layer_input))* del_err_output

# TODO: Calculate change in weights for hidden layer to output layer
delta_w_h_o = learnrate * del_err_output * hidden_layer_output

# TODO: Calculate change in weights for input layer to hidden layer
delta_w_i_o =learnrate * del_err_output * hidden_layer_output

print('Change in weights for hidden layer to output layer:')
print(delta_w_h_o)
print('Change in weights for input layer to hidden layer:')
print(delta_w_i_o)
#+END_SRC

#+RESULTS:

** Implementing Backprop

* Model Evaluatoin and Validation
** Evaluation Metrics
*** Classifications metrics
**** Confusion Matrix
    _____
   |TP|FN|
   |__|__|
   |FP|TN|
   |__|__|

**** Accuracy

*** Regression metrics
**** Mean Absolute Error 
     (not differentiable, so cannot be used with methods like GD)
      
**** Mean Squared Error 
     (differentiable, so can be used with methods like GD)

**** R2 Score (read Prof. Andrew's notes)
     Comparing our model to simplest possible model that fits a bunch of points

** Types of Errors
*** Underfitting (high bias) and Overfitting (high variance)
    
** K-Fold Cross Validation
